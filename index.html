<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Wolfgang Paier</title>

    <meta name="author" content="Wolfgang Paier">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,400;0,9..40,500;0,9..40,600;0,9..40,700&family=Fraunces:opsz,wght@9..144,400;9..144,600;9..144,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>
  <body class="page">
    <table class="layout"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td class="header-title" colspan="2">
                <p class="name">Wolfgang Paier</p>
                <p class="links">
                  <a href="https://www.linkedin.com/in/wolfgang-paier-95869314a/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=uminFHoAAAAJ&hl=de">Google Scholar</a>
                </p>
              </td>
            </tr>
            <tr>
              <td class="header-bio">
                <img class="profile-photo" alt="profile photo" src="images/profile.jpg">
                <p>
                I am a Senior Machine Learning Research Engineer at <a href="https://pipio.ai/">Pipio AI</a>, where I work on foundation video models for talking humans. My focus is on video diffusion models with high-quality lip synchronization, designed for video editing workflows.
                </p><p>
                Previously, I was a Research Associate in the <a href="https://www.hhi.fraunhofer.de/en/departments/computer-vision-and-graphics">Computer Vision &amp; Graphics</a> group at Fraunhofer HHI, where I worked on the generation and animation of 3D talking head models using VAEs, 3DMMs, and neural rendering.
                </p><p>
                I received my PhD from <a href="https://www.hu-berlin.de/en/">HU Berlin</a>, where I was advised by <a href="https://www.informatik.hu-berlin.de/de/forschung/gebiete/viscom/eisert">Peter Eisert</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td class="section">
                <h2>Research</h2>
                <p>
                  My research interests span computer vision, computer graphics, deep learning, and generative AI.
                  Currently, my work focuses on video diffusion models for synthesizing and editing human-centric video content, with an emphasis on realistic and accurate lip synchronization.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td class="section publications">
                <h2>Publications</h2>
                <p style="font-size: 0.9em; color: #555; margin-top: -0.5em;">
                  * Indicates first author or equal contribution
                </p>
                <p class="publication">
                  <strong><a href="https://arxiv.org/pdf/2601.22127">EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers</a></strong>
                  <br>John Flynn*, Wolfgang Paier*, Dimitar Dinev, Sam Nhut Nguyen, Hayk Poghosyan, Manuel Toribio, Sandipan Banerjee, Guy Gafni<br>
                  <em>arXiv:2601.22127</em>, 2026
                </p>
                <p class="publication">
                  <strong><a href="https://arxiv.org/pdf/2403.04380">Video-Driven Animation of Neural Head Avatars</a></strong>
                  <br>Wolfgang Paier*, Paul Hinzer, Anna Hilsmann, Peter Eisert<br>
                  <em>Proc. International Workshop on Vision, Modeling, and Visualization (VMV)</em>, 2024
                </p>
                <p class="publication">
                  <strong><a href="https://edoc.hu-berlin.de/bitstreams/14daaf6c-4872-4321-9e81-b2a61d899fcc/download">Hybrid Methods for the Analysis and Synthesis of Human Faces</a></strong>
                  <br>Wolfgang Paier<br>
                  <em>PhD Thesis</em>, Mathematisch-Naturwissenschaftliche Fakult√§t HU-Berlin, 2024
                </p>
                <p class="publication">
                  <strong><a href="https://arxiv.org/abs/2306.10006">Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances</a></strong>
                  <br>Wolfgang Paier*, Anna Hilsmann, Peter Eisert<br>
                  <em>Graphical Models</em>, 129, 101199, 2023
                </p>
                <p class="publication">
                  <strong><a href="https://ieeexplore.ieee.org/document/9384188">Example-Based Facial Animation of Virtual Reality Avatars Using Auto-Regressive Neural Networks</a></strong>
                  <br>Wolfgang Paier*, Anna Hilsmann, Peter Eisert<br>
                  <em>IEEE Computer Graphics and Applications</em>, 41(4), 52-63, 2021
                </p>
                <p class="publication">
                  <strong><a href="https://dl.acm.org/doi/pdf/10.1145/3429341.3429356">Neural Face Models for Example-Based Visual Speech Synthesis</a></strong>
                  <br>Wolfgang Paier*, Anna Hilsmann, Peter Eisert<br>
                  <em>Proceedings of the 17th ACM SIGGRAPH European conference on visual media production</em>, 1-10, 2020
                </p>
                <p class="publication">
                  <strong><a href="https://ietresearch.onlinelibrary.wiley.com/doi/pdfdirect/10.1049/iet-cvi.2019.0790">Interactive Facial Animation with Deep Neural Networks</a></strong>
                  <br>Wolfgang Paier*, Anna Hilsmann, Peter Eisert<br>
                  <em>IET Computer Vision</em>, 14(6), 359-369, 2020
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td class="footer">
                <p>
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
